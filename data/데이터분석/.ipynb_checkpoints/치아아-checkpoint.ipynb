{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71660b53-27c7-463c-828a-ce067c2c74f7",
   "metadata": {},
   "source": [
    "# ã…‡ã…‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1af67ed-4e77-486a-aa08-5493670f8a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from scipy import stats\n",
    "\n",
    "%matplotlib inline\n",
    "from matplotlib import font_manager, rc\n",
    "\n",
    "font_location = \"C:\\Windows\\Fonts\\malgun.ttf\"\n",
    "font_name = font_manager.FontProperties(fname=font_location).get_name()\n",
    "rc('font', family=font_name)\n",
    "plt.rcParams['axes.unicode_minus'] = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd0bff0-a24f-4a7a-94fc-6dee2b35bb11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0754024-a3c0-4b3d-bdee-3118fccafc84",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 31686, number of negative: 48314\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001423 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 982\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 11\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Number of positive: 31729, number of negative: 48271\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004617 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 865\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 12\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Number of positive: 31694, number of negative: 48306\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001008 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1134\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 13\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Number of positive: 31742, number of negative: 48258\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004215 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 867\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 11\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Number of positive: 31822, number of negative: 48178\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001420 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 894\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 13\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "â€¦ 5/10 done (last AUC=0.715)\n",
      "[LightGBM] [Info] Number of positive: 31777, number of negative: 48223\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002443 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 885\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 13\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 31819, number of negative: 48181\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001448 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 873\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 11\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 31666, number of negative: 48334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003346 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1019\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 13\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Number of positive: 31726, number of negative: 48274\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000947 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1359\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 12\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Number of positive: 31890, number of negative: 48110\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003872 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 986\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 11\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "â€¦ 10/10 done (last AUC=0.720)\n",
      "\n",
      "=== ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ ê²°ê³¼ TOP 5 ===\n",
      "   trial   k                                               cols     AUC\n",
      "8      9  12  [í˜ˆì²­ì§€ì˜¤í‹°(AST), ìŒì£¼ì—¬ë¶€, ì—°ë ¹ëŒ€ì½”ë“œ(5ì„¸ë‹¨ìœ„), ìˆ˜ì¶•ê¸°í˜ˆì••, êµ¬ê°•ê²€ì§„ìˆ˜ê²€ì—¬...  0.7236\n",
      "3      4  11  [ìš”ë‹¨ë°±, ìŒì£¼ì—¬ë¶€, ì—°ë ¹ëŒ€ì½”ë“œ(5ì„¸ë‹¨ìœ„), ìˆ˜ì¶•ê¸°í˜ˆì••, LDLì½œë ˆìŠ¤í…Œë¡¤, ì‹œë ¥(ìš°...  0.7228\n",
      "7      8  13  [ìš”ë‹¨ë°±, ìŒì£¼ì—¬ë¶€, ì—°ë ¹ëŒ€ì½”ë“œ(5ì„¸ë‹¨ìœ„), ì‹œë„ì½”ë“œ, êµ¬ê°•ê²€ì§„ìˆ˜ê²€ì—¬ë¶€, ìˆ˜ì¶•ê¸°í˜ˆì••...  0.7228\n",
      "2      3  13  [í˜ˆì²­ì§€ì˜¤í‹°(AST), ìŒì£¼ì—¬ë¶€, ì—°ë ¹ëŒ€ì½”ë“œ(5ì„¸ë‹¨ìœ„), ì‹œë„ì½”ë“œ, ìˆ˜ì¶•ê¸°í˜ˆì••, ì‹œ...  0.7220\n",
      "0      1  11  [ìŒì£¼ì—¬ë¶€, ì—°ë ¹ëŒ€ì½”ë“œ(5ì„¸ë‹¨ìœ„), ì„±ë³„ì½”ë“œ, ìˆ˜ì¶•ê¸°í˜ˆì••, êµ¬ê°•ê²€ì§„ìˆ˜ê²€ì—¬ë¶€, HDL...  0.7216\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from lightgbm import LGBMClassifier\n",
    "import random\n",
    "\n",
    "# -------------------------\n",
    "# 0) ë°ì´í„° ë¡œë“œ & ë¼ë²¨ ìƒì„±\n",
    "# -------------------------\n",
    "df = pd.read_csv(\"êµ­ë¯¼ê±´ê°•ë³´í—˜ê³µë‹¨_ê±´ê°•ê²€ì§„ì •ë³´_2024.CSV\", encoding=\"cp949\")\n",
    "df[\"ê³ í˜ˆë‹¹_ë¶„ì„ìš©\"] = (df[\"ì‹ì „í˜ˆë‹¹(ê³µë³µí˜ˆë‹¹)\"] >= 100).astype(int)\n",
    "\n",
    "# ğŸ”¹ ë°ì´í„° ìƒ˜í”Œë§ (ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ìš©: 10ë§Œí–‰ë§Œ ì‚¬ìš©)\n",
    "df_work = df.sample(n=100000, random_state=42).copy()\n",
    "\n",
    "# -------------------------\n",
    "# 1) ê¸°ë³¸ ì„¸íŒ…\n",
    "# -------------------------\n",
    "TARGET = \"ê³ í˜ˆë‹¹_ë¶„ì„ìš©\"\n",
    "ID_LIKE = [\"ê¸°ì¤€ë…„ë„\", \"ê°€ì…ìì¼ë ¨ë²ˆí˜¸\"]\n",
    "GLUCOSE_COLS = [\"ì‹ì „í˜ˆë‹¹(ê³µë³µí˜ˆë‹¹)\"]\n",
    "MISSING_THRESH = 0.30\n",
    "LOW_REL_TOP_K = 15\n",
    "N_TRIALS = 10          # ğŸ”¹ ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ìš©: 10íšŒë§Œ\n",
    "K_RANGE = (3, 5)\n",
    "\n",
    "ANCHORS = [\"ì—°ë ¹ëŒ€ì½”ë“œ(5ì„¸ë‹¨ìœ„)\", \"í—ˆë¦¬ë‘˜ë ˆ\", \"ìˆ˜ì¶•ê¸°í˜ˆì••\", \"ì´ì™„ê¸°í˜ˆì••\",\n",
    "           \"HDLì½œë ˆìŠ¤í…Œë¡¤\", \"LDLì½œë ˆìŠ¤í…Œë¡¤\", \"í¡ì—°ìƒíƒœ\", \"ìŒì£¼ì—¬ë¶€\"]\n",
    "ANCHORS = [c for c in ANCHORS if c in df_work.columns]\n",
    "\n",
    "# -------------------------\n",
    "# 2) Feature Pool ì„¤ì •\n",
    "# -------------------------\n",
    "exclude_cols = set([TARGET] + ID_LIKE + [c for c in GLUCOSE_COLS if c in df_work.columns])\n",
    "feature_pool = [c for c in df_work.columns if c not in exclude_cols]\n",
    "\n",
    "# ê²°ì¸¡ì¹˜ 30% ë„˜ëŠ” ì»¬ëŸ¼ ì œì™¸\n",
    "missing_rate = df_work[feature_pool].isna().mean().sort_values()\n",
    "kept_by_missing = missing_rate[missing_rate <= MISSING_THRESH].index.tolist()\n",
    "\n",
    "# -------------------------\n",
    "# 3) ëœë¤ ì¡°í•© í‰ê°€ í•¨ìˆ˜\n",
    "# -------------------------\n",
    "def eval_random_combos(df, pool_cols, y_col, n_trials=10, k_range=(3,5),\n",
    "                       always_include=None, random_state=42, verbose_every=5):\n",
    "    rng = random.Random(random_state)\n",
    "    results = []\n",
    "    pool_cols = [c for c in pool_cols if c not in (always_include or [])]\n",
    "    y = df[y_col].astype(int)\n",
    "\n",
    "    for i in range(1, n_trials+1):\n",
    "        k = rng.randint(k_range[0], k_range[1])\n",
    "        subset = rng.sample(pool_cols, min(k, len(pool_cols)))\n",
    "        if always_include:\n",
    "            subset = list(set(subset + list(always_include)))\n",
    "\n",
    "        X = df[subset].copy()\n",
    "        for c in X.columns:\n",
    "            if X[c].nunique() <= 15:\n",
    "                X[c] = X[c].fillna(X[c].mode().iloc[0])\n",
    "            else:\n",
    "                X[c] = X[c].astype(float).fillna(X[c].median())\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=i\n",
    "        )\n",
    "\n",
    "        neg, pos = (y_train==0).sum(), (y_train==1).sum()\n",
    "        scale = max(1.0, neg / max(1, pos))\n",
    "\n",
    "        model = LGBMClassifier(\n",
    "            n_estimators=100,     # ğŸ”¹ íŠ¸ë¦¬ ê°œìˆ˜ ì¤„ì„ (400 â†’ 100)\n",
    "            learning_rate=0.1,    # ğŸ”¹ ì¡°ê¸ˆ ë†’ì—¬ì„œ ìˆ˜ë ´ ë¹ ë¥´ê²Œ\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            random_state=i,\n",
    "            class_weight={0:1, 1:scale},\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        proba = model.predict_proba(X_test)[:,1]\n",
    "        auc = roc_auc_score(y_test, proba)\n",
    "\n",
    "        results.append({\n",
    "            \"trial\": i,\n",
    "            \"k\": len(subset),\n",
    "            \"cols\": subset,\n",
    "            \"AUC\": round(auc, 4)\n",
    "        })\n",
    "\n",
    "        if verbose_every and i % verbose_every == 0:\n",
    "            print(f\"â€¦ {i}/{n_trials} done (last AUC={auc:.3f})\")\n",
    "\n",
    "    return pd.DataFrame(results).sort_values(by=\"AUC\", ascending=False)\n",
    "\n",
    "# -------------------------\n",
    "# 4) ì‹¤í–‰ ì˜ˆì‹œ\n",
    "# -------------------------\n",
    "res_test = eval_random_combos(\n",
    "    df=df_work,\n",
    "    pool_cols=kept_by_missing,\n",
    "    y_col=TARGET,\n",
    "    n_trials=N_TRIALS,\n",
    "    k_range=K_RANGE,\n",
    "    always_include=ANCHORS,\n",
    "    random_state=123\n",
    ")\n",
    "\n",
    "print(\"\\n=== ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ ê²°ê³¼ TOP 5 ===\")\n",
    "print(res_test.head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d68868af-d52d-4266-a5e0-f9ccdaa7695f",
   "metadata": {},
   "outputs": [],
   "source": [
    "low_rel_candidates = [\n",
    "    \"ì¹˜ì•„ìš°ì‹ì¦ìœ ë¬´\", \"ì¹˜ì„\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b803ac73-2c18-420d-82be-0c76687706a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 31732, number of negative: 48268\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002991 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 848\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 31804, number of negative: 48196\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003708 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 845\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Number of positive: 31828, number of negative: 48172\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002363 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 844\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Number of positive: 31769, number of negative: 48231\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002041 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 846\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 31783, number of negative: 48217\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003241 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 847\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "â€¦ 5/20 done (last AUC=0.717)\n",
      "[LightGBM] [Info] Number of positive: 31827, number of negative: 48173\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002296 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 845\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 31699, number of negative: 48301\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002857 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 844\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Number of positive: 31720, number of negative: 48280\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002024 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 847\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 31756, number of negative: 48244\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002033 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 846\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 31878, number of negative: 48122\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002791 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 844\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "â€¦ 10/20 done (last AUC=0.720)\n",
      "[LightGBM] [Info] Number of positive: 31769, number of negative: 48231\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001884 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 841\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 31814, number of negative: 48186\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003309 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 848\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 31701, number of negative: 48299\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001610 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 845\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 31794, number of negative: 48206\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003002 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 844\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 31789, number of negative: 48211\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003031 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 843\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "â€¦ 15/20 done (last AUC=0.716)\n",
      "[LightGBM] [Info] Number of positive: 31766, number of negative: 48234\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001917 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 845\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 31756, number of negative: 48244\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002054 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 848\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 31840, number of negative: 48160\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001911 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 844\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 31794, number of negative: 48206\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002070 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 843\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 31690, number of negative: 48310\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002364 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 846\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "â€¦ 20/20 done (last AUC=0.720)\n",
      "\n",
      "=== ì¹˜ê³¼ ë³€ìˆ˜ í¬í•¨ (ìƒ˜í”Œ 10ë§Œê±´, TOP ê²°ê³¼ 10) ===\n",
      "    trial   k                                               cols     AUC\n",
      "16     17  10  [ìŒì£¼ì—¬ë¶€, ì—°ë ¹ëŒ€ì½”ë“œ(5ì„¸ë‹¨ìœ„), ìˆ˜ì¶•ê¸°í˜ˆì••, LDLì½œë ˆìŠ¤í…Œë¡¤, HDLì½œë ˆìŠ¤í…Œë¡¤,...  0.7265\n",
      "17     18  10  [ìŒì£¼ì—¬ë¶€, ì—°ë ¹ëŒ€ì½”ë“œ(5ì„¸ë‹¨ìœ„), ìˆ˜ì¶•ê¸°í˜ˆì••, LDLì½œë ˆìŠ¤í…Œë¡¤, HDLì½œë ˆìŠ¤í…Œë¡¤,...  0.7242\n",
      "11     12  10  [ìŒì£¼ì—¬ë¶€, ì—°ë ¹ëŒ€ì½”ë“œ(5ì„¸ë‹¨ìœ„), ìˆ˜ì¶•ê¸°í˜ˆì••, LDLì½œë ˆìŠ¤í…Œë¡¤, HDLì½œë ˆìŠ¤í…Œë¡¤,...  0.7228\n",
      "1       2  10  [ìŒì£¼ì—¬ë¶€, ì—°ë ¹ëŒ€ì½”ë“œ(5ì„¸ë‹¨ìœ„), ìˆ˜ì¶•ê¸°í˜ˆì••, LDLì½œë ˆìŠ¤í…Œë¡¤, HDLì½œë ˆìŠ¤í…Œë¡¤,...  0.7218\n",
      "12     13  10  [ìŒì£¼ì—¬ë¶€, ì—°ë ¹ëŒ€ì½”ë“œ(5ì„¸ë‹¨ìœ„), ìˆ˜ì¶•ê¸°í˜ˆì••, LDLì½œë ˆìŠ¤í…Œë¡¤, HDLì½œë ˆìŠ¤í…Œë¡¤,...  0.7213\n",
      "15     16  10  [ìŒì£¼ì—¬ë¶€, ì—°ë ¹ëŒ€ì½”ë“œ(5ì„¸ë‹¨ìœ„), ìˆ˜ì¶•ê¸°í˜ˆì••, LDLì½œë ˆìŠ¤í…Œë¡¤, HDLì½œë ˆìŠ¤í…Œë¡¤,...  0.7208\n",
      "13     14  10  [ìŒì£¼ì—¬ë¶€, ì—°ë ¹ëŒ€ì½”ë“œ(5ì„¸ë‹¨ìœ„), ìˆ˜ì¶•ê¸°í˜ˆì••, LDLì½œë ˆìŠ¤í…Œë¡¤, HDLì½œë ˆìŠ¤í…Œë¡¤,...  0.7206\n",
      "9      10  10  [ìŒì£¼ì—¬ë¶€, ì—°ë ¹ëŒ€ì½”ë“œ(5ì„¸ë‹¨ìœ„), ìˆ˜ì¶•ê¸°í˜ˆì••, LDLì½œë ˆìŠ¤í…Œë¡¤, HDLì½œë ˆìŠ¤í…Œë¡¤,...  0.7202\n",
      "19     20  10  [ìŒì£¼ì—¬ë¶€, ì—°ë ¹ëŒ€ì½”ë“œ(5ì„¸ë‹¨ìœ„), ìˆ˜ì¶•ê¸°í˜ˆì••, LDLì½œë ˆìŠ¤í…Œë¡¤, HDLì½œë ˆìŠ¤í…Œë¡¤,...  0.7200\n",
      "3       4  10  [ìŒì£¼ì—¬ë¶€, ì—°ë ¹ëŒ€ì½”ë“œ(5ì„¸ë‹¨ìœ„), ìˆ˜ì¶•ê¸°í˜ˆì••, LDLì½œë ˆìŠ¤í…Œë¡¤, HDLì½œë ˆìŠ¤í…Œë¡¤,...  0.7198\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# 0) ìƒ˜í”Œë§ (10ë§Œê±´)\n",
    "# =========================\n",
    "df_sample = df_work.sample(n=100000, random_state=42)\n",
    "\n",
    "# =========================\n",
    "# 1) ì¹˜ê³¼ ì»¬ëŸ¼ í¬í•¨ ëœë¤ ì¡°í•©\n",
    "# =========================\n",
    "DENTAL_EXTRA = [c for c in [\"ì¹˜ì•„ìš°ì‹ì¦ìœ ë¬´\", \"ì¹˜ì„\"] if c in df_sample.columns]\n",
    "\n",
    "res_with_dental = eval_random_combos(\n",
    "    df=df_sample,\n",
    "    pool_cols=low_rel_candidates,    \n",
    "    y_col=TARGET,\n",
    "    n_trials=20,                     # ë¹ ë¥¸ í™•ì¸ â†’ 20ë²ˆë§Œ\n",
    "    k_range=(5, 8),                  # ì¡°í•© í¬ê¸° 5~8\n",
    "    always_include=ANCHORS + DENTAL_EXTRA,   \n",
    "    random_state=123\n",
    ")\n",
    "\n",
    "print(\"\\n=== ì¹˜ê³¼ ë³€ìˆ˜ í¬í•¨ (ìƒ˜í”Œ 10ë§Œê±´, TOP ê²°ê³¼ 10) ===\")\n",
    "print(res_with_dental.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f4e4a3c-62b9-430b-b2a2-96872d1221e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\OWNER\\anaconda3\\envs\\prophet\\lib\\site-packages\\sklearn\\impute\\_base.py:558: UserWarning: Skipping features without any observed values: ['ê²°ì†ì¹˜ ìœ ë¬´' 'ì¹˜ì•„ë§ˆëª¨ì¦ìœ ë¬´' 'ì œ3ëŒ€êµ¬ì¹˜(ì‚¬ë‘ë‹ˆ) ì´ìƒ']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 317970, number of negative: 482030\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.091563 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2794\n",
      "[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 29\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.397462 -> initscore=-0.416049\n",
      "[LightGBM] [Info] Start training from score -0.416049\n",
      "=== LightGBM (í’€ í”¼ì²˜) ===\n",
      "AUC: 0.7466083990804482\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\OWNER\\anaconda3\\envs\\prophet\\lib\\site-packages\\shap\\explainers\\_tree.py:583: UserWarning: LightGBM binary classifier with TreeExplainer shap values output has changed to a list of ndarray\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== SHAP Top 10 í”¼ì²˜ ===\n",
      "          feature  shap_importance\n",
      "0            ê¸°ì¤€ë…„ë„          0.05528\n",
      "17            í˜ˆìƒ‰ì†Œ          0.05528\n",
      "31       í¡ì—°ìƒíƒœ_3.0          0.05528\n",
      "30       í¡ì—°ìƒíƒœ_2.0          0.05528\n",
      "29         ì„±ë³„ì½”ë“œ_2          0.05528\n",
      "28             ì¹˜ì„          0.05528\n",
      "27  ì œ3ëŒ€êµ¬ì¹˜(ì‚¬ë‘ë‹ˆ) ì´ìƒ          0.05528\n",
      "26        ì¹˜ì•„ë§ˆëª¨ì¦ìœ ë¬´          0.05528\n",
      "25         ê²°ì†ì¹˜ ìœ ë¬´          0.05528\n",
      "24        ì¹˜ì•„ìš°ì‹ì¦ìœ ë¬´          0.05528\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "All arrays must be of the same length",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 66\u001b[0m\n\u001b[0;32m     63\u001b[0m rfe \u001b[38;5;241m=\u001b[39m RFE(log_model, n_features_to_select\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m     64\u001b[0m rfe\u001b[38;5;241m.\u001b[39mfit(X_scaled, y)\n\u001b[1;32m---> 66\u001b[0m rfe_support \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfeature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mselected\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mrfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msupport_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mranking\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mrfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mranking_\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msort_values(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mranking\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m=== RFE Top 10 í”¼ì²˜ ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28mprint\u001b[39m(rfe_support[rfe_support[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mselected\u001b[39m\u001b[38;5;124m\"\u001b[39m]])\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\prophet\\lib\\site-packages\\pandas\\core\\frame.py:664\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    658\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_mgr(\n\u001b[0;32m    659\u001b[0m         data, axes\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m: index, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: columns}, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy\n\u001b[0;32m    660\u001b[0m     )\n\u001b[0;32m    662\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m    663\u001b[0m     \u001b[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[1;32m--> 664\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[43mdict_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    665\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ma\u001b[38;5;241m.\u001b[39mMaskedArray):\n\u001b[0;32m    666\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mma\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmrecords\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmrecords\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\prophet\\lib\\site-packages\\pandas\\core\\internals\\construction.py:493\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[1;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[0;32m    489\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    490\u001b[0m         \u001b[38;5;66;03m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[0;32m    491\u001b[0m         arrays \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[1;32m--> 493\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marrays_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\prophet\\lib\\site-packages\\pandas\\core\\internals\\construction.py:118\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[1;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verify_integrity:\n\u001b[0;32m    116\u001b[0m     \u001b[38;5;66;03m# figure out the index, if necessary\u001b[39;00m\n\u001b[0;32m    117\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 118\u001b[0m         index \u001b[38;5;241m=\u001b[39m \u001b[43m_extract_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    119\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    120\u001b[0m         index \u001b[38;5;241m=\u001b[39m ensure_index(index)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\prophet\\lib\\site-packages\\pandas\\core\\internals\\construction.py:666\u001b[0m, in \u001b[0;36m_extract_index\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    664\u001b[0m lengths \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(raw_lengths))\n\u001b[0;32m    665\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(lengths) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 666\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll arrays must be of the same length\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    668\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m have_dicts:\n\u001b[0;32m    669\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    670\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMixing dicts with non-Series may lead to ambiguous ordering.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    671\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: All arrays must be of the same length"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, f1_score, accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import RFE\n",
    "from lightgbm import LGBMClassifier\n",
    "import shap\n",
    "\n",
    "# -----------------------------\n",
    "# ë°ì´í„° ì¤€ë¹„\n",
    "# -----------------------------\n",
    "target = \"ê³ í˜ˆë‹¹_ë¶„ì„ìš©\"\n",
    "exclude_cols = [\"ê³ í˜ˆë‹¹_ë¶„ì„ìš©\", \"ê³ í˜ˆë‹¹_ì„œë¹„ìŠ¤ìš©\", \"ì‹ì „í˜ˆë‹¹(ê³µë³µí˜ˆë‹¹)\", \"risk_level\"]\n",
    "X = df.drop(columns=exclude_cols, errors=\"ignore\")\n",
    "y = df[target].astype(int)\n",
    "\n",
    "cat_cols = [\"ì„±ë³„ì½”ë“œ\", \"í¡ì—°ìƒíƒœ\", \"ìŒì£¼ì—¬ë¶€\"]\n",
    "X = pd.get_dummies(X, columns=cat_cols, drop_first=True)\n",
    "\n",
    "# ê²°ì¸¡ì¹˜ ì²˜ë¦¬ + ìŠ¤ì¼€ì¼ë§\n",
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "scaler = StandardScaler()\n",
    "X_imputed = imputer.fit_transform(X)\n",
    "X_scaled = scaler.fit_transform(X_imputed)\n",
    "\n",
    "# -----------------------------\n",
    "# LightGBM (í’€ í”¼ì²˜)\n",
    "# -----------------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "lgb_model = LGBMClassifier(\n",
    "    n_estimators=300,\n",
    "    learning_rate=0.05,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42\n",
    ")\n",
    "lgb_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_proba = lgb_model.predict_proba(X_test)[:, 1]\n",
    "print(\"=== LightGBM (í’€ í”¼ì²˜) ===\")\n",
    "print(\"AUC:\", roc_auc_score(y_test, y_pred_proba))\n",
    "\n",
    "# -----------------------------\n",
    "# SHAP ê¸°ë°˜ ì¤‘ìš”ë„ Top 10\n",
    "# -----------------------------\n",
    "explainer = shap.TreeExplainer(lgb_model)\n",
    "shap_values = explainer.shap_values(X_test[:1000])  # ìƒ˜í”Œ ì¼ë¶€ë§Œ (ì†ë„ ë¬¸ì œ ë°©ì§€)\n",
    "\n",
    "shap_importance = np.abs(shap_values[1]).mean(axis=0)  # í´ë˜ìŠ¤=1 ê¸°ì¤€\n",
    "shap_df = pd.DataFrame({\"feature\": X.columns, \"shap_importance\": shap_importance})\n",
    "shap_top10 = shap_df.sort_values(\"shap_importance\", ascending=False).head(10)\n",
    "print(\"\\n=== SHAP Top 10 í”¼ì²˜ ===\")\n",
    "print(shap_top10)\n",
    "\n",
    "# -----------------------------\n",
    "# RFE ê¸°ë°˜ ì¤‘ìš” í”¼ì²˜\n",
    "# -----------------------------\n",
    "log_model = LogisticRegression(max_iter=500, solver=\"liblinear\")\n",
    "rfe = RFE(log_model, n_features_to_select=10)\n",
    "rfe.fit(X_scaled, y)\n",
    "\n",
    "rfe_support = pd.DataFrame({\n",
    "    \"feature\": X.columns,\n",
    "    \"selected\": rfe.support_,\n",
    "    \"ranking\": rfe.ranking_\n",
    "}).sort_values(\"ranking\")\n",
    "\n",
    "print(\"\\n=== RFE Top 10 í”¼ì²˜ ===\")\n",
    "print(rfe_support[rfe_support[\"selected\"]])\n",
    "\n",
    "# -----------------------------\n",
    "# Top í”¼ì²˜ë§Œìœ¼ë¡œ ë‹¤ì‹œ ëª¨ë¸ í•™ìŠµ\n",
    "# -----------------------------\n",
    "top_features = shap_top10[\"feature\"].tolist()\n",
    "X_train_top, X_test_top = X_train[top_features], X_test[top_features]\n",
    "\n",
    "lgb_model_top = LGBMClassifier(\n",
    "    n_estimators=300,\n",
    "    learning_rate=0.05,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42\n",
    ")\n",
    "lgb_model_top.fit(X_train_top, y_train)\n",
    "\n",
    "y_pred_proba_top = lgb_model_top.predict_proba(X_test_top)[:, 1]\n",
    "print(\"\\n=== LightGBM (SHAP Top 10 í”¼ì²˜ë§Œ) ===\")\n",
    "print(\"AUC:\", roc_auc_score(y_test, y_pred_proba_top))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b7f2bf-e06d-4c88-b1f8-3888a6f54191",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from lightgbm import LGBMClassifier\n",
    "import shap\n",
    "\n",
    "# -----------------------------\n",
    "# 1. í”¼ì²˜ / ë¼ë²¨ ë¶„ë¦¬\n",
    "# -----------------------------\n",
    "target = \"ê³ í˜ˆë‹¹_ë¶„ì„ìš©\"\n",
    "exclude_cols = [\"ê³ í˜ˆë‹¹_ë¶„ì„ìš©\", \"ì‹ì „í˜ˆë‹¹(ê³µë³µí˜ˆë‹¹)\", \"ê³ í˜ˆë‹¹_ì„œë¹„ìŠ¤ìš©\", \"risk_level\"]\n",
    "X = df.drop(columns=exclude_cols, errors=\"ignore\")\n",
    "y = df[target].astype(int)\n",
    "\n",
    "# -----------------------------\n",
    "# 2. ê²°ì¸¡ì¹˜ ì²˜ë¦¬ (null ì œê±°)\n",
    "# -----------------------------\n",
    "# ë²”ì£¼í˜• í›„ë³´\n",
    "cat_cols = [\"ì„±ë³„ì½”ë“œ\", \"í¡ì—°ìƒíƒœ\", \"ìŒì£¼ì—¬ë¶€\"]\n",
    "num_cols = [c for c in X.columns if c not in cat_cols]\n",
    "\n",
    "# ìˆ˜ì¹˜í˜• â†’ ì¤‘ì•™ê°’\n",
    "for col in num_cols:\n",
    "    if col in X.columns:\n",
    "        X[col] = X[col].fillna(X[col].median())\n",
    "\n",
    "# ë²”ì£¼í˜• â†’ ìµœë¹ˆê°’\n",
    "for col in cat_cols:\n",
    "    if col in X.columns:\n",
    "        X[col] = X[col].fillna(X[col].mode()[0])\n",
    "\n",
    "# ì›-í•« ì¸ì½”ë”©\n",
    "X = pd.get_dummies(X, columns=cat_cols, drop_first=True)\n",
    "\n",
    "# ìŠ¤ì¼€ì¼ë§ (ì˜µì…˜: ë¡œì§€ìŠ¤í‹± ëŒ€ë¹„ìš©, LGBMì€ ì•ˆ í•´ë„ OK)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# -----------------------------\n",
    "# 3. ëª¨ë¸ í•™ìŠµ\n",
    "# -----------------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "lgb = LGBMClassifier(\n",
    "    n_estimators=200,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=6,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42\n",
    ")\n",
    "lgb.fit(X_train, y_train)\n",
    "\n",
    "# -----------------------------\n",
    "# 4. SHAP ê³„ì‚°\n",
    "# -----------------------------\n",
    "explainer = shap.TreeExplainer(lgb)\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "\n",
    "# ì¤‘ìš”ë„ í‰ê· \n",
    "shap_importance = np.abs(shap_values).mean(axis=0)\n",
    "importance_df = pd.DataFrame({\n",
    "    \"feature\": X.columns,\n",
    "    \"shap_importance\": shap_importance\n",
    "}).sort_values(by=\"shap_importance\", ascending=False)\n",
    "\n",
    "print(\"=== SHAP Top 10 í”¼ì²˜ ===\")\n",
    "print(importance_df.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624c43f2-b28d-4cdd-92d9-f23644ae690f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Nullë§Œ ìˆëŠ” ì»¬ëŸ¼ ì œê±° (ì¤‘ìš”!)\n",
    "# -----------------------------\n",
    "X = X.dropna(axis=1, how=\"all\")   # ì „ë¶€ NaNì¸ ì»¬ëŸ¼ ì‚­ì œ\n",
    "\n",
    "# ê²°ì¸¡ì¹˜ ì²˜ë¦¬ + ìŠ¤ì¼€ì¼ë§\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_imputed = imputer.fit_transform(X)\n",
    "X_scaled = scaler.fit_transform(X_imputed)\n",
    "\n",
    "# ë‹¤ì‹œ DataFrameìœ¼ë¡œ (RFE/SHAPì—ì„œ ì»¬ëŸ¼ëª… ë³´ì¡´)\n",
    "X_scaled_df = pd.DataFrame(X_scaled, columns=X.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ad567cc-eb1e-4616-b502-61696810119a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ ì „ë¶€ NaNì¸ ì»¬ëŸ¼: ['ê²°ì†ì¹˜ ìœ ë¬´', 'ì¹˜ì•„ë§ˆëª¨ì¦ìœ ë¬´', 'ì œ3ëŒ€êµ¬ì¹˜(ì‚¬ë‘ë‹ˆ) ì´ìƒ']\n",
      "\n",
      "=== RFE Top 10 í”¼ì²˜ ===\n",
      "        feature  selected  ranking\n",
      "22        ê°ë§ˆì§€í‹°í”¼      True        1\n",
      "21   í˜ˆì²­ì§€í”¼í‹°(ALT)      True        1\n",
      "3   ì—°ë ¹ëŒ€ì½”ë“œ(5ì„¸ë‹¨ìœ„)      True        1\n",
      "20   í˜ˆì²­ì§€ì˜¤í‹°(AST)      True        1\n",
      "5     ì²´ì¤‘(5kgë‹¨ìœ„)      True        1\n",
      "6          í—ˆë¦¬ë‘˜ë ˆ      True        1\n",
      "17          í˜ˆìƒ‰ì†Œ      True        1\n",
      "16     LDLì½œë ˆìŠ¤í…Œë¡¤      True        1\n",
      "11        ìˆ˜ì¶•ê¸°í˜ˆì••      True        1\n",
      "13       ì´ì½œë ˆìŠ¤í…Œë¡¤      True        1\n",
      "[LightGBM] [Info] Number of positive: 317970, number of negative: 482030\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.066355 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2789\n",
      "[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 29\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.397462 -> initscore=-0.416049\n",
      "[LightGBM] [Info] Start training from score -0.416049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\OWNER\\anaconda3\\envs\\prophet\\lib\\site-packages\\shap\\explainers\\_tree.py:583: UserWarning: LightGBM binary classifier with TreeExplainer shap values output has changed to a list of ndarray\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== SHAP Top 10 í”¼ì²˜ ===\n",
      "        feature  shap_importance\n",
      "3   ì—°ë ¹ëŒ€ì½”ë“œ(5ì„¸ë‹¨ìœ„)         0.476083\n",
      "6          í—ˆë¦¬ë‘˜ë ˆ         0.253645\n",
      "22        ê°ë§ˆì§€í‹°í”¼         0.238442\n",
      "11        ìˆ˜ì¶•ê¸°í˜ˆì••         0.132355\n",
      "21   í˜ˆì²­ì§€í”¼í‹°(ALT)         0.110055\n",
      "20   í˜ˆì²­ì§€ì˜¤í‹°(AST)         0.104809\n",
      "19      í˜ˆì²­í¬ë ˆì•„í‹°ë‹Œ         0.063159\n",
      "17          í˜ˆìƒ‰ì†Œ         0.053821\n",
      "14     íŠ¸ë¦¬ê¸€ë¦¬ì„¸ë¼ì´ë“œ         0.051776\n",
      "5     ì²´ì¤‘(5kgë‹¨ìœ„)         0.043946\n",
      "\n",
      "=== LightGBM ì„±ëŠ¥ ===\n",
      "ACC = 0.688\n",
      "F1  = 0.582\n",
      "AUC = 0.745\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_selection import RFE\n",
    "from lightgbm import LGBMClassifier\n",
    "import shap\n",
    "\n",
    "# -----------------------------\n",
    "# 0. ë°ì´í„° ì¤€ë¹„\n",
    "# -----------------------------\n",
    "# (ìœ ë¹ˆë‹˜ CSV ë¶ˆëŸ¬ì˜¤ê¸°)\n",
    "df = pd.read_csv(\"êµ­ë¯¼ê±´ê°•ë³´í—˜ê³µë‹¨_ê±´ê°•ê²€ì§„ì •ë³´_2024.CSV\", encoding=\"cp949\")\n",
    "\n",
    "# ë¼ë²¨ ìƒì„± (100 ì´ìƒ â†’ 1)\n",
    "df[\"ê³ í˜ˆë‹¹_ë¶„ì„ìš©\"] = (df[\"ì‹ì „í˜ˆë‹¹(ê³µë³µí˜ˆë‹¹)\"] >= 100).astype(int)\n",
    "\n",
    "target = \"ê³ í˜ˆë‹¹_ë¶„ì„ìš©\"\n",
    "y = df[target]\n",
    "\n",
    "# ì œì™¸í•  ì»¬ëŸ¼\n",
    "exclude_cols = [\"ê³ í˜ˆë‹¹_ë¶„ì„ìš©\", \"ê³ í˜ˆë‹¹_ì„œë¹„ìŠ¤ìš©\", \"ì‹ì „í˜ˆë‹¹(ê³µë³µí˜ˆë‹¹)\", \"risk_level\"]\n",
    "X = df.drop(columns=exclude_cols, errors=\"ignore\")\n",
    "\n",
    "# -----------------------------\n",
    "# 1. ì „ë¶€ NaN ì»¬ëŸ¼ ì œê±°\n",
    "# -----------------------------\n",
    "null_only_cols = [c for c in X.columns if X[c].isna().all()]\n",
    "print(\"âš ï¸ ì „ë¶€ NaNì¸ ì»¬ëŸ¼:\", null_only_cols)\n",
    "\n",
    "X = X.drop(columns=null_only_cols)\n",
    "\n",
    "# -----------------------------\n",
    "# 2. ë²”ì£¼í˜• ì›-í•« ì¸ì½”ë”©\n",
    "# -----------------------------\n",
    "cat_cols = [\"ì„±ë³„ì½”ë“œ\", \"í¡ì—°ìƒíƒœ\", \"ìŒì£¼ì—¬ë¶€\"]\n",
    "X = pd.get_dummies(X, columns=[c for c in cat_cols if c in X.columns], drop_first=True)\n",
    "\n",
    "# -----------------------------\n",
    "# 3. ê²°ì¸¡ì¹˜ ì²˜ë¦¬ + ìŠ¤ì¼€ì¼ë§\n",
    "# -----------------------------\n",
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_imputed = imputer.fit_transform(X)\n",
    "X_scaled = scaler.fit_transform(X_imputed)\n",
    "\n",
    "X_scaled_df = pd.DataFrame(X_scaled, columns=X.columns)\n",
    "y = y.astype(int)\n",
    "\n",
    "# -----------------------------\n",
    "# 4. RFE (Logistic Regression ê¸°ë°˜)\n",
    "# -----------------------------\n",
    "log_model = LogisticRegression(max_iter=500, solver=\"liblinear\")\n",
    "rfe = RFE(log_model, n_features_to_select=10)\n",
    "rfe.fit(X_scaled_df, y)\n",
    "\n",
    "rfe_support = pd.DataFrame({\n",
    "    \"feature\": X.columns,\n",
    "    \"selected\": rfe.support_,\n",
    "    \"ranking\": rfe.ranking_\n",
    "}).sort_values(\"ranking\")\n",
    "\n",
    "print(\"\\n=== RFE Top 10 í”¼ì²˜ ===\")\n",
    "print(rfe_support[rfe_support[\"selected\"]])\n",
    "\n",
    "# -----------------------------\n",
    "# 5. LightGBM + SHAP\n",
    "# -----------------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled_df, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "lgb_model = LGBMClassifier(\n",
    "    n_estimators=200,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=6,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "lgb_model.fit(X_train, y_train)\n",
    "\n",
    "# SHAP ê°’ ê³„ì‚°\n",
    "explainer = shap.TreeExplainer(lgb_model)\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "\n",
    "shap_importance = np.abs(shap_values).mean(axis=0)\n",
    "shap_df = pd.DataFrame({\n",
    "    \"feature\": X.columns,\n",
    "    \"shap_importance\": shap_importance\n",
    "}).sort_values(\"shap_importance\", ascending=False)\n",
    "\n",
    "print(\"\\n=== SHAP Top 10 í”¼ì²˜ ===\")\n",
    "print(shap_df.head(10))\n",
    "\n",
    "# -----------------------------\n",
    "# 6. ì„±ëŠ¥ í™•ì¸\n",
    "# -----------------------------\n",
    "y_pred = lgb_model.predict(X_test)\n",
    "y_proba = lgb_model.predict_proba(X_test)[:,1]\n",
    "\n",
    "print(\"\\n=== LightGBM ì„±ëŠ¥ ===\")\n",
    "print(f\"ACC = {accuracy_score(y_test, y_pred):.3f}\")\n",
    "print(f\"F1  = {f1_score(y_test, y_pred):.3f}\")\n",
    "print(f\"AUC = {roc_auc_score(y_test, y_proba):.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "49a978d5-a0cc-43fb-aa88-6d5567c9226b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Threshold  Precision  Recall     F1\n",
      "0       0.30      0.523   0.850  0.648\n",
      "1       0.40      0.569   0.727  0.639\n",
      "2       0.50      0.623   0.551  0.585\n",
      "3       0.55      0.653   0.446  0.530\n",
      "4       0.60      0.685   0.331  0.446\n",
      "5       0.65      0.721   0.219  0.335\n",
      "6       0.70      0.759   0.121  0.208\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# ì´ë¯¸ y_test, y_pred_proba (í™•ë¥ ) ê°€ ìˆë‹¤ê³  ê°€ì •\n",
    "thresholds = [0.3, 0.4, 0.5, 0.55, 0.6, 0.65, 0.7]\n",
    "\n",
    "results = []\n",
    "for t in thresholds:\n",
    "    y_pred = (y_pred_proba >= t).astype(int)\n",
    "    prec = precision_score(y_test, y_pred)\n",
    "    rec = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    results.append({\n",
    "        \"Threshold\": t,\n",
    "        \"Precision\": round(prec, 3),\n",
    "        \"Recall\": round(rec, 3),\n",
    "        \"F1\": round(f1, 3)\n",
    "    })\n",
    "\n",
    "df_thresh = pd.DataFrame(results)\n",
    "print(df_thresh)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f9651d-678a-445a-bd51-f058b28a6db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 1. BMI ì¶”ê°€\n",
    "# -----------------------------\n",
    "df[\"BMI\"] = df[\"ì²´ì¤‘(5kgë‹¨ìœ„)\"] * 5 / ((df[\"ì‹ ì¥(5cmë‹¨ìœ„)\"]*5 / 100) ** 2)\n",
    "\n",
    "# -----------------------------\n",
    "# 2. í”¼ì²˜/íƒ€ê¹ƒ ë¶„ë¦¬\n",
    "# -----------------------------\n",
    "target = \"ê³ í˜ˆë‹¹_ë¶„ì„ìš©\"\n",
    "y = df[target].astype(int)\n",
    "\n",
    "exclude_cols = [\"ê³ í˜ˆë‹¹_ë¶„ì„ìš©\", \"ê³ í˜ˆë‹¹_ì„œë¹„ìŠ¤ìš©\", \"ì‹ì „í˜ˆë‹¹(ê³µë³µí˜ˆë‹¹)\", \"risk_level\"]\n",
    "X = df.drop(columns=exclude_cols, errors=\"ignore\")\n",
    "\n",
    "cat_cols = [\"ì„±ë³„ì½”ë“œ\", \"í¡ì—°ìƒíƒœ\", \"ìŒì£¼ì—¬ë¶€\"]\n",
    "X = pd.get_dummies(X, columns=cat_cols, drop_first=True)\n",
    "X = X.astype(\"float32\").fillna(0)\n",
    "\n",
    "# -----------------------------\n",
    "# 3. ëª¨ë¸ í•™ìŠµ\n",
    "# -----------------------------\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "neg, pos = (y_train == 0).sum(), (y_train == 1).sum()\n",
    "scale = neg / pos if pos > 0 else 1\n",
    "\n",
    "model = LGBMClassifier(\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.05,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    class_weight={0: 1, 1: scale},\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# ì˜ˆì¸¡ í™•ë¥ \n",
    "y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# ì„±ëŠ¥ í™•ì¸\n",
    "y_pred = (y_pred_proba >= 0.5).astype(int)\n",
    "prec = precision_score(y_test, y_pred)\n",
    "rec = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "print(\"=== BMI ì¶”ê°€ í›„ ëª¨ë¸ ì„±ëŠ¥ ===\")\n",
    "print(f\"Precision: {prec:.3f}, Recall: {rec:.3f}, F1: {f1:.3f}, AUC: {auc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a0d479-2b9b-469f-b8d1-c554e23b77ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# -----------------------------\n",
    "# 1. í˜ˆì••ì°¨ ì¶”ê°€\n",
    "# -----------------------------\n",
    "df[\"í˜ˆì••ì°¨\"] = df[\"ìˆ˜ì¶•ê¸°í˜ˆì••\"] - df[\"ì´ì™„ê¸°í˜ˆì••\"]\n",
    "\n",
    "# -----------------------------\n",
    "# 2. í”¼ì²˜/íƒ€ê¹ƒ ë¶„ë¦¬\n",
    "# -----------------------------\n",
    "target = \"ê³ í˜ˆë‹¹_ë¶„ì„ìš©\"\n",
    "y = df[target].astype(int)\n",
    "\n",
    "exclude_cols = [\"ê³ í˜ˆë‹¹_ë¶„ì„ìš©\", \"ê³ í˜ˆë‹¹_ì„œë¹„ìŠ¤ìš©\", \"ì‹ì „í˜ˆë‹¹(ê³µë³µí˜ˆë‹¹)\", \"risk_level\"]\n",
    "X = df.drop(columns=exclude_cols, errors=\"ignore\")\n",
    "\n",
    "cat_cols = [\"ì„±ë³„ì½”ë“œ\", \"í¡ì—°ìƒíƒœ\", \"ìŒì£¼ì—¬ë¶€\"]\n",
    "X = pd.get_dummies(X, columns=cat_cols, drop_first=True)\n",
    "X = X.astype(\"float32\").fillna(0)\n",
    "\n",
    "# -----------------------------\n",
    "# 3. train-test split\n",
    "# -----------------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "neg, pos = (y_train == 0).sum(), (y_train == 1).sum()\n",
    "scale = neg / pos if pos > 0 else 1\n",
    "\n",
    "# -----------------------------\n",
    "# 4. LightGBM í•™ìŠµ\n",
    "# -----------------------------\n",
    "model = LGBMClassifier(\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.05,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    class_weight={0: 1, 1: scale},\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# -----------------------------\n",
    "# 5. ì„±ëŠ¥ í‰ê°€\n",
    "# -----------------------------\n",
    "y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "y_pred = (y_pred_proba >= 0.5).astype(int)\n",
    "\n",
    "prec = precision_score(y_test, y_pred)\n",
    "rec = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "print(\"=== í˜ˆì••ì°¨ ì¶”ê°€ í›„ ëª¨ë¸ ì„±ëŠ¥ ===\")\n",
    "print(f\"Precision: {prec:.3f}, Recall: {rec:.3f}, F1: {f1:.3f}, AUC: {auc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0cc37a77-171f-4714-a3d2-7dacc27b38bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 317970, number of negative: 482030\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.040490 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3292\n",
      "[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "Threshold=0.30 â†’ Precision=0.486, Recall=0.917, F1=0.636, AUC=0.747\n",
      "Threshold=0.40 â†’ Precision=0.527, Recall=0.843, F1=0.648, AUC=0.747\n",
      "Threshold=0.50 â†’ Precision=0.570, Recall=0.728, F1=0.639, AUC=0.747\n",
      "Threshold=0.60 â†’ Precision=0.622, Recall=0.557, F1=0.588, AUC=0.747\n",
      "Threshold=0.70 â†’ Precision=0.688, Recall=0.319, F1=0.436, AUC=0.747\n"
     ]
    }
   ],
   "source": [
    "df[\"ì½œë ˆìŠ¤í…Œë¡¤ë¹„ìœ¨\"] = df[\"ì´ì½œë ˆìŠ¤í…Œë¡¤\"] / (df[\"HDLì½œë ˆìŠ¤í…Œë¡¤\"] + 1e-6)\n",
    "df[\"AST_ALTë¹„ìœ¨\"] = df[\"í˜ˆì²­ì§€ì˜¤í‹°(AST)\"] / (df[\"í˜ˆì²­ì§€í”¼í‹°(ALT)\"] + 1e-6)\n",
    "\n",
    "# -----------------------------\n",
    "# 2. í”¼ì²˜/íƒ€ê¹ƒ ë¶„ë¦¬\n",
    "# -----------------------------\n",
    "target = \"ê³ í˜ˆë‹¹_ë¶„ì„ìš©\"\n",
    "y = df[target].astype(int)\n",
    "\n",
    "exclude_cols = [\"ê³ í˜ˆë‹¹_ë¶„ì„ìš©\",\"ê³ í˜ˆë‹¹_ì„œë¹„ìŠ¤ìš©\",\"ì‹ì „í˜ˆë‹¹(ê³µë³µí˜ˆë‹¹)\",\"risk_level\"]\n",
    "X = df.drop(columns=exclude_cols, errors=\"ignore\")\n",
    "\n",
    "cat_cols = [\"ì„±ë³„ì½”ë“œ\",\"í¡ì—°ìƒíƒœ\",\"ìŒì£¼ì—¬ë¶€\"]\n",
    "X = pd.get_dummies(X, columns=cat_cols, drop_first=True)\n",
    "X = X.astype(\"float32\").fillna(0)\n",
    "\n",
    "# -----------------------------\n",
    "# 3. ëª¨ë¸ í•™ìŠµ & Threshold ì‹¤í—˜\n",
    "# -----------------------------\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "neg, pos = (y_train==0).sum(), (y_train==1).sum()\n",
    "scale = neg / pos if pos > 0 else 1\n",
    "\n",
    "model = LGBMClassifier(\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.05,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    class_weight={0:1, 1:scale},\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# ì˜ˆì¸¡ í™•ë¥ \n",
    "y_pred_proba = model.predict_proba(X_test)[:,1]\n",
    "\n",
    "# ì—¬ëŸ¬ Thresholdì—ì„œ ì„±ëŠ¥ í™•ì¸\n",
    "thresholds = [0.3, 0.4, 0.5, 0.6, 0.7]\n",
    "for t in thresholds:\n",
    "    y_pred = (y_pred_proba >= t).astype(int)\n",
    "    prec = precision_score(y_test, y_pred)\n",
    "    rec = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    print(f\"Threshold={t:.2f} â†’ Precision={prec:.3f}, Recall={rec:.3f}, F1={f1:.3f}, AUC={auc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2a2b509a-7727-4725-8450-7dd7e1c14614",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 317970, number of negative: 482030\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.041794 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3606\n",
      "[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 33\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "Threshold=0.30 â†’ Precision=0.487, Recall=0.917, F1=0.636, AUC=0.747\n",
      "Threshold=0.40 â†’ Precision=0.527, Recall=0.842, F1=0.648, AUC=0.747\n",
      "Threshold=0.50 â†’ Precision=0.570, Recall=0.727, F1=0.639, AUC=0.747\n",
      "Threshold=0.60 â†’ Precision=0.622, Recall=0.556, F1=0.587, AUC=0.747\n",
      "Threshold=0.70 â†’ Precision=0.688, Recall=0.319, F1=0.436, AUC=0.747\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# 1. ìƒˆë¡œìš´ íŒŒìƒë³€ìˆ˜ ìƒì„±\n",
    "# -----------------------------\n",
    "df[\"BMI\"] = df[\"ì²´ì¤‘(5kgë‹¨ìœ„)\"] * 5 / ((df[\"ì‹ ì¥(5cmë‹¨ìœ„)\"]*5 / 100) ** 2)   # ë‹¨ìœ„ ë³´ì •\n",
    "df[\"í˜ˆì••ì°¨\"] = df[\"ìˆ˜ì¶•ê¸°í˜ˆì••\"] - df[\"ì´ì™„ê¸°í˜ˆì••\"]\n",
    "df[\"ì½œë ˆìŠ¤í…Œë¡¤ë¹„ìœ¨\"] = df[\"ì´ì½œë ˆìŠ¤í…Œë¡¤\"] / (df[\"HDLì½œë ˆìŠ¤í…Œë¡¤\"] + 1e-6)\n",
    "df[\"AST_ALTë¹„ìœ¨\"] = df[\"í˜ˆì²­ì§€ì˜¤í‹°(AST)\"] / (df[\"í˜ˆì²­ì§€í”¼í‹°(ALT)\"] + 1e-6)\n",
    "# ì…ë ¥ê°€ëŠ¥í•œ í”¼ì³ë§Œ ì¶”ê°€í•´ì„œ  ì„±ëŠ¥ í–¥ìƒìœ¼ë¡œ \n",
    "\n",
    "# -----------------------------\n",
    "# 2. í”¼ì²˜/íƒ€ê¹ƒ ë¶„ë¦¬\n",
    "# -----------------------------\n",
    "target = \"ê³ í˜ˆë‹¹_ë¶„ì„ìš©\"\n",
    "y = df[target].astype(int)\n",
    "\n",
    "exclude_cols = [\"ê³ í˜ˆë‹¹_ë¶„ì„ìš©\",\"ê³ í˜ˆë‹¹_ì„œë¹„ìŠ¤ìš©\",\"ì‹ì „í˜ˆë‹¹(ê³µë³µí˜ˆë‹¹)\",\"risk_level\"]\n",
    "X = df.drop(columns=exclude_cols, errors=\"ignore\")\n",
    "\n",
    "cat_cols = [\"ì„±ë³„ì½”ë“œ\",\"í¡ì—°ìƒíƒœ\",\"ìŒì£¼ì—¬ë¶€\"]\n",
    "X = pd.get_dummies(X, columns=cat_cols, drop_first=True)\n",
    "X = X.astype(\"float32\").fillna(0)\n",
    "\n",
    "# -----------------------------\n",
    "# 3. ëª¨ë¸ í•™ìŠµ & Threshold ì‹¤í—˜\n",
    "# -----------------------------\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "neg, pos = (y_train==0).sum(), (y_train==1).sum()\n",
    "scale = neg / pos if pos > 0 else 1\n",
    "\n",
    "model = LGBMClassifier(\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.05,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    class_weight={0:1, 1:scale},\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# ì˜ˆì¸¡ í™•ë¥ \n",
    "y_pred_proba = model.predict_proba(X_test)[:,1]\n",
    "\n",
    "# ì—¬ëŸ¬ Thresholdì—ì„œ ì„±ëŠ¥ í™•ì¸\n",
    "thresholds = [0.3, 0.4, 0.5, 0.6, 0.7]\n",
    "for t in thresholds:\n",
    "    y_pred = (y_pred_proba >= t).astype(int)\n",
    "    prec = precision_score(y_test, y_pred)\n",
    "    rec = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    print(f\"Threshold={t:.2f} â†’ Precision={prec:.3f}, Recall={rec:.3f}, F1={f1:.3f}, AUC={auc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ce7ae890-92b3-49b0-b780-1d6238931f18",
   "metadata": {},
   "source": [
    "Threshold=0.40 â†’ Precision=0.527, Recall=0.841, F1=0.648, AUC=0.747\n",
    "ìµœì ì˜ ê²°ê³¼ ì–»ìŒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4083eff0-113c-426a-9d2b-60f2d01af2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "# -----------------------------\n",
    "# 1. ëª¨ë¸ í•™ìŠµ (ì´ë¯¸ í•˜ì‹  ë¶€ë¶„)\n",
    "# -----------------------------\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# -----------------------------\n",
    "# 2. ëª¨ë¸ & ë©”íƒ€ë°ì´í„° ì €ì¥\n",
    "# -----------------------------\n",
    "meta = {\n",
    "    \"features\": X.columns.tolist(),   # í”¼ì²˜ ìˆœì„œ ì €ì¥\n",
    "    \"median_values\": X.median().to_dict()  # ì¤‘ì•™ê°’ ì €ì¥\n",
    "}\n",
    "\n",
    "with open(\"glucose_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump((model, meta), f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028643ea-8b44-42b2-9d3b-f6a5068f74f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b281761-5263-4b86-9eeb-b7132fe2eabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í˜ˆë‹¹ íˆìŠ¤í† ê·¸ë¨\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.hist(df[\"ì‹ì „í˜ˆë‹¹(ê³µë³µí˜ˆë‹¹)\"], bins=30, color=\"skyblue\", edgecolor=\"black\")\n",
    "plt.title(\"ì‹ì „í˜ˆë‹¹ ë¶„í¬\")\n",
    "plt.xlabel(\"í˜ˆë‹¹ (mg/dL)\")\n",
    "plt.ylabel(\"ë¹ˆë„ìˆ˜\")\n",
    "plt.axvline(100, color=\"red\", linestyle=\"--\", label=\"ê¸°ì¤€ì¹˜ 100\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# ì¹˜ì•„ ìš°ì‹ì¦ íˆìŠ¤í† ê·¸ë¨\n",
    "plt.figure(figsize=(6,4))\n",
    "df[\"ì¹˜ì•„ìš°ì‹ì¦ìœ ë¬´\"].value_counts().plot(kind=\"bar\", color=\"tomato\")\n",
    "plt.title(\"ì¹˜ì•„ìš°ì‹ì¦ ìœ ë¬´ ë¶„í¬\")\n",
    "plt.xlabel(\"ì¹˜ì•„ìš°ì‹ì¦ ìœ ë¬´ (0=ì—†ìŒ, 1=ìˆìŒ)\")\n",
    "plt.ylabel(\"ì¸ì›ìˆ˜\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d7e958-0eef-490a-ba99-1b37fd7c28d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0adc51d3-8828-43df-8959-502655932e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171831c8-5519-4f1a-a712-45fe659f6186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë¶„ì„ìš© ë³€ìˆ˜ ì„ íƒ\n",
    "features = df[[\"ì—°ë ¹ëŒ€ì½”ë“œ(5ì„¸ë‹¨ìœ„)\", \"BMI\", \"ì‹ì „í˜ˆë‹¹(ê³µë³µí˜ˆë‹¹)\", \"ì´ì½œë ˆìŠ¤í…Œë¡¤\"]].dropna()\n",
    "\n",
    "# í‘œì¤€í™”\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(features)\n",
    "\n",
    "# K-means í´ëŸ¬ìŠ¤í„°ë§\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "features[\"cluster\"] = kmeans.fit_predict(X)\n",
    "\n",
    "# ì‹œê°í™” (2D ì‚°ì ë„)\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.scatterplot(data=features, x=\"BMI\", y=\"ì‹ì „í˜ˆë‹¹(ê³µë³µí˜ˆë‹¹)\", hue=\"cluster\", palette=\"Set2\")\n",
    "plt.title(\"í´ëŸ¬ìŠ¤í„°ë§: BMI vs í˜ˆë‹¹\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fb3dda17-ee5e-49b0-aeae-f93c30d28276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 317970, number of negative: 482030\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.038568 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3606\n",
      "[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 33\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "Threshold=0.30 â†’ Precision=0.487, Recall=0.917, F1=0.636, AUC=0.747\n",
      "Threshold=0.40 â†’ Precision=0.527, Recall=0.842, F1=0.648, AUC=0.747\n",
      "Threshold=0.50 â†’ Precision=0.570, Recall=0.728, F1=0.640, AUC=0.747\n",
      "Threshold=0.60 â†’ Precision=0.623, Recall=0.556, F1=0.588, AUC=0.747\n",
      "Threshold=0.70 â†’ Precision=0.689, Recall=0.321, F1=0.438, AUC=0.747\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# 1. ìƒˆë¡œìš´ íŒŒìƒë³€ìˆ˜ ìƒì„±\n",
    "# -----------------------------\n",
    "df[\"BMI\"] = df[\"ì²´ì¤‘(5kgë‹¨ìœ„)\"] * 5 / ((df[\"ì‹ ì¥(5cmë‹¨ìœ„)\"]*5 / 100) ** 2)   # ë‹¨ìœ„ ë³´ì •\n",
    "df[\"í˜ˆì••ì°¨\"] = df[\"ìˆ˜ì¶•ê¸°í˜ˆì••\"] - df[\"ì´ì™„ê¸°í˜ˆì••\"]\n",
    "df[\"ì½œë ˆìŠ¤í…Œë¡¤ë¹„ìœ¨\"] = df[\"ì´ì½œë ˆìŠ¤í…Œë¡¤\"] / (df[\"HDLì½œë ˆìŠ¤í…Œë¡¤\"] + 1e-6)\n",
    "df[\"AST_ALTë¹„ìœ¨\"] = df[\"í˜ˆì²­ì§€ì˜¤í‹°(AST)\"] / (df[\"í˜ˆì²­ì§€í”¼í‹°(ALT)\"] + 1e-6)\n",
    "\n",
    "# -----------------------------\n",
    "# 2. í”¼ì²˜/íƒ€ê¹ƒ ë¶„ë¦¬\n",
    "# -----------------------------\n",
    "target = \"ê³ í˜ˆë‹¹_ë¶„ì„ìš©\"\n",
    "y = df[target].astype(int)\n",
    "\n",
    "exclude_cols = [\"ê³ í˜ˆë‹¹_ë¶„ì„ìš©\",\"ê³ í˜ˆë‹¹_ì„œë¹„ìŠ¤ìš©\",\"ì‹ì „í˜ˆë‹¹(ê³µë³µí˜ˆë‹¹)\",\"risk_level\"]\n",
    "\n",
    "# ì¹˜ì•„ ê´€ë ¨ ë³€ìˆ˜ ê°•ì œ í¬í•¨\n",
    "dental_cols = [c for c in [\"ì¹˜ì•„ìš°ì‹ì¦ìœ ë¬´\"] if c in df.columns]\n",
    "\n",
    "X = df.drop(columns=exclude_cols, errors=\"ignore\")\n",
    "\n",
    "# ì¹´í…Œê³ ë¦¬í˜• ë³€ìˆ˜ ì¸ì½”ë”©\n",
    "cat_cols = [\"ì„±ë³„ì½”ë“œ\",\"í¡ì—°ìƒíƒœ\",\"ìŒì£¼ì—¬ë¶€\"] + dental_cols\n",
    "X = pd.get_dummies(X, columns=cat_cols, drop_first=True)\n",
    "\n",
    "# float32ë¡œ ë³€í™˜ + ê²°ì¸¡ê°’ ì²˜ë¦¬\n",
    "X = X.astype(\"float32\").fillna(0)\n",
    "\n",
    "# -----------------------------\n",
    "# 3. ëª¨ë¸ í•™ìŠµ & Threshold ì‹¤í—˜\n",
    "# -----------------------------\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "neg, pos = (y_train==0).sum(), (y_train==1).sum()\n",
    "scale = neg / pos if pos > 0 else 1\n",
    "\n",
    "model = LGBMClassifier(\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.05,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    class_weight={0:1, 1:scale},\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# ì˜ˆì¸¡ í™•ë¥ \n",
    "y_pred_proba = model.predict_proba(X_test)[:,1]\n",
    "\n",
    "# ì—¬ëŸ¬ Thresholdì—ì„œ ì„±ëŠ¥ í™•ì¸\n",
    "thresholds = [0.3, 0.4, 0.5, 0.6, 0.7]\n",
    "for t in thresholds:\n",
    "    y_pred = (y_pred_proba >= t).astype(int) \n",
    "    prec = precision_score(y_test, y_pred)\n",
    "    rec = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    print(f\"Threshold={t:.2f} â†’ Precision={prec:.3f}, Recall={rec:.3f}, F1={f1:.3f}, AUC={auc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e8c37c-1244-44ad-bbab-48447aead0f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
